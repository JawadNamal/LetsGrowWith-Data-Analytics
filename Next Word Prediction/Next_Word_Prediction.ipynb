{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FTWk3_Mkfgy"
      },
      "source": [
        "##**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1n7lpfRho7B"
      },
      "outputs": [],
      "source": [
        "#Import required libraries\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygqrAOhEka5o"
      },
      "source": [
        "##**Reading The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGHhmZhbiCkE",
        "outputId": "36fa20c3-b648-42c9-c341-73a536d49f05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<http.client.HTTPResponse at 0x7f722df4a250>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the dataset\n",
        "file = urllib.request.urlopen(('https://raw.githubusercontent.com/Bharath-K3/Next-Word-Prediction-with-NLP-and-Deep-Learning/master/metamorphosis_clean.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpgmdRNHiSwb"
      },
      "outputs": [],
      "source": [
        "lines = []\n",
        "for i in file:\n",
        "  decoded_line = i.decode(\"utf-8\")\n",
        "  lines.append(decoded_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNz927bAkPY1",
        "outputId": "8d02ffce-584c-4d88-b42e-d40465079bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
            "\n",
            "The Last Line:  first to get up and stretch out her young body.\n"
          ]
        }
      ],
      "source": [
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGUfL-sSkTAo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_qQqA3xkl4v"
      },
      "source": [
        "##**Cleaning The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FryteqkIks46",
        "outputId": "25bc174a-93c2-480c-d2f6-27a55a8eb174"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "NwJTk1eqkvdE",
        "outputId": "2542cf0f-a209-42a9-c8f8-f307d1cf2fd7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "48lcgzFUkzYs",
        "outputId": "078fa09c-5b1f-4a06-d2c8-00d0488e7d96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rJ5PAqKk5Yf"
      },
      "source": [
        "##**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hisj2ZpJk2r8",
        "outputId": "ec907840-b537-41a1-8903-03378147a857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R063WJLTk8x9",
        "outputId": "b934d240-2487-47e7-f19e-c7d373f6246a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2617\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD0Vh-5UlAWE",
        "outputId": "61639c95-9b29-4ab2-caa5-b5660a77e7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3LnrszLlCb-"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV4H0rs2lExm",
        "outputId": "8fc10fa1-4ef6-4db2-ab69-29548eb40046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Data is:  [ 17  53 293   2  18]\n",
            "The responses are:  [ 53 293   2  18 729]\n"
          ]
        }
      ],
      "source": [
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdHyb0UqlGc2",
        "outputId": "baa665c0-3f96-432a-b097-80f5a8adaef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQXzAwZRlKkA"
      },
      "source": [
        "##**Creating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWpobaZ-lIoG"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZDABOV3lOuf",
        "outputId": "5b024a99-81e2-431c-b20b-bb0c3d19aaea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "MhS8RdTXlQV_",
        "outputId": "a4ccae32-526f-4866-9305-5fe29ae1821d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bJJfQRQli-9"
      },
      "source": [
        "##**Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9ZsyufLlejS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKjG8ETSlq1j"
      },
      "source": [
        "##**Compile The Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pwoorYPloxT",
        "outputId": "a32f32da-5b3b-424d-eb5d-c6f16b0b4d53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44u7dF_5lv3x"
      },
      "source": [
        "##**Fit The Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pElZK5Mlt-k",
        "outputId": "4ce52d8d-5f96-495a-c1cf-6d83a37b0dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8750\n",
            "Epoch 1: loss improved from inf to 7.87501, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 202ms/step - loss: 7.8750 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8622\n",
            "Epoch 2: loss improved from 7.87501 to 7.86216, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 7.8622 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8238\n",
            "Epoch 3: loss improved from 7.86216 to 7.82383, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 7.8238 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6360\n",
            "Epoch 4: loss improved from 7.82383 to 7.63603, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 7.6360 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4325\n",
            "Epoch 5: loss improved from 7.63603 to 7.43248, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 7.4325 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.2627\n",
            "Epoch 6: loss improved from 7.43248 to 7.26269, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 211ms/step - loss: 7.2627 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1474\n",
            "Epoch 7: loss improved from 7.26269 to 7.14738, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 7.1474 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0627\n",
            "Epoch 8: loss improved from 7.14738 to 7.06271, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 7.0627 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9841\n",
            "Epoch 9: loss improved from 7.06271 to 6.98412, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 202ms/step - loss: 6.9841 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.8915\n",
            "Epoch 10: loss improved from 6.98412 to 6.89151, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 6.8915 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.7447\n",
            "Epoch 11: loss improved from 6.89151 to 6.74470, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 6.7447 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.5190\n",
            "Epoch 12: loss improved from 6.74470 to 6.51902, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 6.5190 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.2999\n",
            "Epoch 13: loss improved from 6.51902 to 6.29991, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 6.2999 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.0876\n",
            "Epoch 14: loss improved from 6.29991 to 6.08761, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 6.0876 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.9081\n",
            "Epoch 15: loss improved from 6.08761 to 5.90808, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 5.9081 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.7166\n",
            "Epoch 16: loss improved from 5.90808 to 5.71660, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 5.7166 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5714\n",
            "Epoch 17: loss improved from 5.71660 to 5.57141, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 5.5714 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4271\n",
            "Epoch 18: loss improved from 5.57141 to 5.42714, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 5.4271 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2873\n",
            "Epoch 19: loss improved from 5.42714 to 5.28725, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 5.2873 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1528\n",
            "Epoch 20: loss improved from 5.28725 to 5.15276, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 5.1528 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0365\n",
            "Epoch 21: loss improved from 5.15276 to 5.03653, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 5.0365 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9093\n",
            "Epoch 22: loss improved from 5.03653 to 4.90933, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 4.9093 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8637\n",
            "Epoch 23: loss improved from 4.90933 to 4.86372, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.8637 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7306\n",
            "Epoch 24: loss improved from 4.86372 to 4.73056, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.7306 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6825\n",
            "Epoch 25: loss improved from 4.73056 to 4.68245, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.6825 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6185\n",
            "Epoch 26: loss improved from 4.68245 to 4.61846, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.6185 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5160\n",
            "Epoch 27: loss improved from 4.61846 to 4.51598, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 4.5160 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4459\n",
            "Epoch 28: loss improved from 4.51598 to 4.44591, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.4459 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3649\n",
            "Epoch 29: loss improved from 4.44591 to 4.36492, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.3649 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2923\n",
            "Epoch 30: loss improved from 4.36492 to 4.29229, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 4.2923 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2533\n",
            "Epoch 31: loss improved from 4.29229 to 4.25334, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.2533 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2164\n",
            "Epoch 32: loss improved from 4.25334 to 4.21638, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 4.2164 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1708\n",
            "Epoch 33: loss improved from 4.21638 to 4.17076, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 4.1708 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1369\n",
            "Epoch 34: loss improved from 4.17076 to 4.13687, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.1369 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0672\n",
            "Epoch 35: loss improved from 4.13687 to 4.06724, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 4.0672 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9850\n",
            "Epoch 36: loss improved from 4.06724 to 3.98495, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 3.9850 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9352\n",
            "Epoch 37: loss improved from 3.98495 to 3.93517, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 3.9352 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9119\n",
            "Epoch 38: loss improved from 3.93517 to 3.91193, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 3.9119 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8380\n",
            "Epoch 39: loss improved from 3.91193 to 3.83803, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 3.8380 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7927\n",
            "Epoch 40: loss improved from 3.83803 to 3.79272, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 3.7927 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7455\n",
            "Epoch 41: loss improved from 3.79272 to 3.74546, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 3.7455 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7092\n",
            "Epoch 42: loss improved from 3.74546 to 3.70917, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 3.7092 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6433\n",
            "Epoch 43: loss improved from 3.70917 to 3.64330, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 3.6433 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5867\n",
            "Epoch 44: loss improved from 3.64330 to 3.58674, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 205ms/step - loss: 3.5867 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5326\n",
            "Epoch 45: loss improved from 3.58674 to 3.53263, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 205ms/step - loss: 3.5326 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4446\n",
            "Epoch 46: loss improved from 3.53263 to 3.44456, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 3.4446 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3224\n",
            "Epoch 47: loss improved from 3.44456 to 3.32243, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 3.3224 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2250\n",
            "Epoch 48: loss improved from 3.32243 to 3.22501, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 205ms/step - loss: 3.2250 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1341\n",
            "Epoch 49: loss improved from 3.22501 to 3.13410, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 3.1341 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0356\n",
            "Epoch 50: loss improved from 3.13410 to 3.03562, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 3.0356 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9827\n",
            "Epoch 51: loss improved from 3.03562 to 2.98271, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 2.9827 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8992\n",
            "Epoch 52: loss improved from 2.98271 to 2.89919, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 2.8992 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8399\n",
            "Epoch 53: loss improved from 2.89919 to 2.83987, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 2.8399 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7545\n",
            "Epoch 54: loss improved from 2.83987 to 2.75446, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 2.7545 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6617\n",
            "Epoch 55: loss improved from 2.75446 to 2.66168, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 2.6617 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5948\n",
            "Epoch 56: loss improved from 2.66168 to 2.59476, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 2.5948 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5321\n",
            "Epoch 57: loss improved from 2.59476 to 2.53209, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 2.5321 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5222\n",
            "Epoch 58: loss improved from 2.53209 to 2.52221, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 2.5222 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4447\n",
            "Epoch 59: loss improved from 2.52221 to 2.44473, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 2.4447 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3993\n",
            "Epoch 60: loss improved from 2.44473 to 2.39930, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 2.3993 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3239\n",
            "Epoch 61: loss improved from 2.39930 to 2.32391, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 2.3239 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2881\n",
            "Epoch 62: loss improved from 2.32391 to 2.28807, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 2.2881 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2651\n",
            "Epoch 63: loss improved from 2.28807 to 2.26508, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 2.2651 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2460\n",
            "Epoch 64: loss improved from 2.26508 to 2.24595, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 210ms/step - loss: 2.2460 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1735\n",
            "Epoch 65: loss improved from 2.24595 to 2.17350, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 211ms/step - loss: 2.1735 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1424\n",
            "Epoch 66: loss improved from 2.17350 to 2.14237, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 2.1424 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0999\n",
            "Epoch 67: loss improved from 2.14237 to 2.09993, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 2.0999 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0397\n",
            "Epoch 68: loss improved from 2.09993 to 2.03967, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 2.0397 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0223\n",
            "Epoch 69: loss improved from 2.03967 to 2.02228, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 2.0223 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0072\n",
            "Epoch 70: loss improved from 2.02228 to 2.00721, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 2.0072 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0056\n",
            "Epoch 71: loss improved from 2.00721 to 2.00557, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 2.0056 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9474\n",
            "Epoch 72: loss improved from 2.00557 to 1.94744, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.9474 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8834\n",
            "Epoch 73: loss improved from 1.94744 to 1.88341, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.8834 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8751\n",
            "Epoch 74: loss improved from 1.88341 to 1.87509, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.8751 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8559\n",
            "Epoch 75: loss improved from 1.87509 to 1.85591, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.8559 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7926\n",
            "Epoch 76: loss improved from 1.85591 to 1.79263, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.7926 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7408\n",
            "Epoch 77: loss improved from 1.79263 to 1.74083, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.7408 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7141\n",
            "Epoch 78: loss improved from 1.74083 to 1.71411, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.7141 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6849\n",
            "Epoch 79: loss improved from 1.71411 to 1.68492, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.6849 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6367\n",
            "Epoch 80: loss improved from 1.68492 to 1.63670, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.6367 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6094\n",
            "Epoch 81: loss improved from 1.63670 to 1.60941, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.6094 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5964\n",
            "Epoch 82: loss improved from 1.60941 to 1.59641, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 211ms/step - loss: 1.5964 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5329\n",
            "Epoch 83: loss improved from 1.59641 to 1.53289, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 210ms/step - loss: 1.5329 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5001\n",
            "Epoch 84: loss improved from 1.53289 to 1.50010, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 220ms/step - loss: 1.5001 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4579\n",
            "Epoch 85: loss improved from 1.50010 to 1.45791, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.4579 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4500\n",
            "Epoch 86: loss improved from 1.45791 to 1.45001, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.4500 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4348\n",
            "Epoch 87: loss improved from 1.45001 to 1.43479, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.4348 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4137\n",
            "Epoch 88: loss improved from 1.43479 to 1.41373, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.4137 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3635\n",
            "Epoch 89: loss improved from 1.41373 to 1.36351, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.3635 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3613\n",
            "Epoch 90: loss improved from 1.36351 to 1.36130, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.3613 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3567\n",
            "Epoch 91: loss improved from 1.36130 to 1.35666, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.3567 - lr: 0.0010\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3581\n",
            "Epoch 92: loss did not improve from 1.35666\n",
            "61/61 [==============================] - 12s 200ms/step - loss: 1.3581 - lr: 0.0010\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3444\n",
            "Epoch 93: loss improved from 1.35666 to 1.34441, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.3444 - lr: 0.0010\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3238\n",
            "Epoch 94: loss improved from 1.34441 to 1.32382, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 226ms/step - loss: 1.3238 - lr: 0.0010\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2795\n",
            "Epoch 95: loss improved from 1.32382 to 1.27953, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.2795 - lr: 0.0010\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2761\n",
            "Epoch 96: loss improved from 1.27953 to 1.27615, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.2761 - lr: 0.0010\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2501\n",
            "Epoch 97: loss improved from 1.27615 to 1.25014, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.2501 - lr: 0.0010\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2198\n",
            "Epoch 98: loss improved from 1.25014 to 1.21978, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.2198 - lr: 0.0010\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2106\n",
            "Epoch 99: loss improved from 1.21978 to 1.21057, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.2106 - lr: 0.0010\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1958\n",
            "Epoch 100: loss improved from 1.21057 to 1.19576, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.1958 - lr: 0.0010\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1901\n",
            "Epoch 101: loss improved from 1.19576 to 1.19012, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.1901 - lr: 0.0010\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1839\n",
            "Epoch 102: loss improved from 1.19012 to 1.18387, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.1839 - lr: 0.0010\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1839\n",
            "Epoch 103: loss did not improve from 1.18387\n",
            "61/61 [==============================] - 12s 200ms/step - loss: 1.1839 - lr: 0.0010\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1682\n",
            "Epoch 104: loss improved from 1.18387 to 1.16821, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.1682 - lr: 0.0010\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1854\n",
            "Epoch 105: loss did not improve from 1.16821\n",
            "61/61 [==============================] - 12s 199ms/step - loss: 1.1854 - lr: 0.0010\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1538\n",
            "Epoch 106: loss improved from 1.16821 to 1.15381, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 1.1538 - lr: 0.0010\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1449\n",
            "Epoch 107: loss improved from 1.15381 to 1.14489, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 1.1449 - lr: 0.0010\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1247\n",
            "Epoch 108: loss improved from 1.14489 to 1.12467, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 1.1247 - lr: 0.0010\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1151\n",
            "Epoch 109: loss improved from 1.12467 to 1.11508, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 1.1151 - lr: 0.0010\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1140\n",
            "Epoch 110: loss improved from 1.11508 to 1.11403, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.1140 - lr: 0.0010\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0958\n",
            "Epoch 111: loss improved from 1.11403 to 1.09578, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.0958 - lr: 0.0010\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1007\n",
            "Epoch 112: loss did not improve from 1.09578\n",
            "61/61 [==============================] - 12s 198ms/step - loss: 1.1007 - lr: 0.0010\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0789\n",
            "Epoch 113: loss improved from 1.09578 to 1.07890, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.0789 - lr: 0.0010\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0767\n",
            "Epoch 114: loss improved from 1.07890 to 1.07674, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.0767 - lr: 0.0010\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0542\n",
            "Epoch 115: loss improved from 1.07674 to 1.05420, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.0542 - lr: 0.0010\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0514\n",
            "Epoch 116: loss improved from 1.05420 to 1.05144, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.0514 - lr: 0.0010\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0539\n",
            "Epoch 117: loss did not improve from 1.05144\n",
            "61/61 [==============================] - 12s 198ms/step - loss: 1.0539 - lr: 0.0010\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0456\n",
            "Epoch 118: loss improved from 1.05144 to 1.04558, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 1.0456 - lr: 0.0010\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0396\n",
            "Epoch 119: loss improved from 1.04558 to 1.03965, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 1.0396 - lr: 0.0010\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0402\n",
            "Epoch 120: loss did not improve from 1.03965\n",
            "61/61 [==============================] - 12s 197ms/step - loss: 1.0402 - lr: 0.0010\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0333\n",
            "Epoch 121: loss improved from 1.03965 to 1.03332, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 205ms/step - loss: 1.0333 - lr: 0.0010\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0530\n",
            "Epoch 122: loss did not improve from 1.03332\n",
            "61/61 [==============================] - 12s 198ms/step - loss: 1.0530 - lr: 0.0010\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0551\n",
            "Epoch 123: loss did not improve from 1.03332\n",
            "61/61 [==============================] - 12s 198ms/step - loss: 1.0551 - lr: 0.0010\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0790\n",
            "Epoch 124: loss did not improve from 1.03332\n",
            "\n",
            "Epoch 124: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 12s 197ms/step - loss: 1.0790 - lr: 0.0010\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8271\n",
            "Epoch 125: loss improved from 1.03332 to 0.82713, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 0.8271 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7365\n",
            "Epoch 126: loss improved from 0.82713 to 0.73653, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 207ms/step - loss: 0.7365 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7113\n",
            "Epoch 127: loss improved from 0.73653 to 0.71127, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 0.7113 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6992\n",
            "Epoch 128: loss improved from 0.71127 to 0.69919, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 0.6992 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6929\n",
            "Epoch 129: loss improved from 0.69919 to 0.69293, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 0.6929 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6884\n",
            "Epoch 130: loss improved from 0.69293 to 0.68840, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 0.6884 - lr: 2.0000e-04\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6849\n",
            "Epoch 131: loss improved from 0.68840 to 0.68493, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 0.6849 - lr: 2.0000e-04\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6833\n",
            "Epoch 132: loss improved from 0.68493 to 0.68334, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 206ms/step - loss: 0.6833 - lr: 2.0000e-04\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6830\n",
            "Epoch 133: loss improved from 0.68334 to 0.68298, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 0.6830 - lr: 2.0000e-04\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6813\n",
            "Epoch 134: loss improved from 0.68298 to 0.68130, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 0.6813 - lr: 2.0000e-04\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6800\n",
            "Epoch 135: loss improved from 0.68130 to 0.67997, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 0.6800 - lr: 2.0000e-04\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6795\n",
            "Epoch 136: loss improved from 0.67997 to 0.67946, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 205ms/step - loss: 0.6795 - lr: 2.0000e-04\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6772\n",
            "Epoch 137: loss improved from 0.67946 to 0.67724, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 0.6772 - lr: 2.0000e-04\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6783\n",
            "Epoch 138: loss did not improve from 0.67724\n",
            "61/61 [==============================] - 12s 197ms/step - loss: 0.6783 - lr: 2.0000e-04\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6785\n",
            "Epoch 139: loss did not improve from 0.67724\n",
            "61/61 [==============================] - 12s 197ms/step - loss: 0.6785 - lr: 2.0000e-04\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6768\n",
            "Epoch 140: loss improved from 0.67724 to 0.67678, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 205ms/step - loss: 0.6768 - lr: 2.0000e-04\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6761\n",
            "Epoch 141: loss improved from 0.67678 to 0.67606, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 204ms/step - loss: 0.6761 - lr: 2.0000e-04\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6772\n",
            "Epoch 142: loss did not improve from 0.67606\n",
            "61/61 [==============================] - 12s 198ms/step - loss: 0.6772 - lr: 2.0000e-04\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6747\n",
            "Epoch 143: loss improved from 0.67606 to 0.67468, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 0.6747 - lr: 2.0000e-04\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6781\n",
            "Epoch 144: loss did not improve from 0.67468\n",
            "61/61 [==============================] - 12s 197ms/step - loss: 0.6781 - lr: 2.0000e-04\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6785\n",
            "Epoch 145: loss did not improve from 0.67468\n",
            "61/61 [==============================] - 12s 196ms/step - loss: 0.6785 - lr: 2.0000e-04\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6746\n",
            "Epoch 146: loss improved from 0.67468 to 0.67462, saving model to nextword1.h5\n",
            "\n",
            "Epoch 146: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 0.6746 - lr: 2.0000e-04\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6372\n",
            "Epoch 147: loss improved from 0.67462 to 0.63719, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 202ms/step - loss: 0.6372 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6343\n",
            "Epoch 148: loss improved from 0.63719 to 0.63432, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 0.6343 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6336\n",
            "Epoch 149: loss improved from 0.63432 to 0.63361, saving model to nextword1.h5\n",
            "61/61 [==============================] - 12s 203ms/step - loss: 0.6336 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6342\n",
            "Epoch 150: loss did not improve from 0.63361\n",
            "61/61 [==============================] - 12s 195ms/step - loss: 0.6342 - lr: 1.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f71b8663090>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIvSL2O6l_U4"
      },
      "source": [
        "##**Graph:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W8TdoQnl1q8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image \n",
        "pil_img = Image(filename='graph1.png')\n",
        "display(pil_img)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Next_Word_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}